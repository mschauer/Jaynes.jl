var documenterSearchIndex = {"docs":
[{"location":"library_api/sites/","page":"Traces, choices, and call sites","title":"Traces, choices, and call sites","text":"CurrentModule = Jaynes","category":"page"},{"location":"library_api/sites/","page":"Traces, choices, and call sites","title":"Traces, choices, and call sites","text":"Trace","category":"page"},{"location":"library_api/sites/#Jaynes.Trace","page":"Traces, choices, and call sites","title":"Jaynes.Trace","text":"abstract type Trace end\n\nAbstract base type of all traces.\n\n\n\n\n\n","category":"type"},{"location":"library_api/sites/","page":"Traces, choices, and call sites","title":"Traces, choices, and call sites","text":"HierarchicalTrace","category":"page"},{"location":"library_api/sites/#Jaynes.HierarchicalTrace","page":"Traces, choices, and call sites","title":"Jaynes.HierarchicalTrace","text":"struct HierarchicalTrace <: Trace\n    calls::Dict{Address, CallSite}\n    choices::Dict{Address, ChoiceSite}\n    params::Dict{Address, LearnableSite}\n    end\nend\n\nExecution trace which allows the tracking of randomness metadata in a function call.\n\n\n\n\n\n","category":"type"},{"location":"library_api/sites/","page":"Traces, choices, and call sites","title":"Traces, choices, and call sites","text":"VectorizedTrace","category":"page"},{"location":"library_api/sites/#Jaynes.VectorizedTrace","page":"Traces, choices, and call sites","title":"Jaynes.VectorizedTrace","text":"struct VectorizedTrace{C <: RecordSite} <: Trace\n    subrecords::Vector{C}\n    params::Dict{Address, LearnableSite}\nend\n\n\nStructured execution trace for markov and plate calls. The dependency structure interpretation of the subrecords vector depends on the call. For markov, the structure is Markovian. For plate, each element is drawn IID from the program or distribution provided to plate.\n\n\n\n\n\n","category":"type"},{"location":"library_api/sites/","page":"Traces, choices, and call sites","title":"Traces, choices, and call sites","text":"ChoiceSite","category":"page"},{"location":"library_api/sites/#Jaynes.ChoiceSite","page":"Traces, choices, and call sites","title":"Jaynes.ChoiceSite","text":"struct ChoiceSite{T} <: RecordSite\n    score::Float64\n    val::T\nend\n\nA record of a random sample at an addressed rand call with a Distribution instance. Keeps the value of the random sample and the logpdf score.\n\n\n\n\n\n","category":"type"},{"location":"library_api/sites/","page":"Traces, choices, and call sites","title":"Traces, choices, and call sites","text":"HierarchicalCallSite","category":"page"},{"location":"library_api/sites/#Jaynes.HierarchicalCallSite","page":"Traces, choices, and call sites","title":"Jaynes.HierarchicalCallSite","text":"struct HierarchicalCallSite{J, K} <: CallSite\n    trace::HierarchicalTrace\n    score::Float64\n    fn::Function\n    args::J\n    ret::K\nend\n\nA record of a black-box call (e.g. no special tracer language features). Records the fn and args for the call, as well as the ret return value.\n\n\n\n\n\n","category":"type"},{"location":"library_api/sites/","page":"Traces, choices, and call sites","title":"Traces, choices, and call sites","text":"VectorizedCallSite","category":"page"},{"location":"library_api/sites/#Jaynes.VectorizedCallSite","page":"Traces, choices, and call sites","title":"Jaynes.VectorizedCallSite","text":"struct VectorizedCallSite{F <: Function, C <: RecordSite, J, K} <: CallSite\n    trace::VectorizedTrace{C}\n    score::Float64\n    fn::Function\n    args::J\n    ret::Vector{K}\nend\n\nA record of a call site using the special plate and markov tracer language features. Informs the tracer that the call conforms to a special pattern of randomness dependency, which allows the storing of Trace instances sequentially in a vector.\n\n\n\n\n\n","category":"type"},{"location":"inference/pf/","page":"Particle filtering","title":"Particle filtering","text":"CurrentModule = Jaynes","category":"page"},{"location":"inference/pf/","page":"Particle filtering","title":"Particle filtering","text":"initialize_filter\nfilter_step!","category":"page"},{"location":"inference/pf/#Jaynes.initialize_filter","page":"Particle filtering","title":"Jaynes.initialize_filter","text":"particles = initialize_filter(observations::ConstrainedHierarchicalSelection,\n                              num_particles::Int,\n                              fn::Function, \n                              args::Tuple)\n\nInstantiate a set of particles using a call to importance_sampling.\n\n\n\n\n\n","category":"function"},{"location":"inference/pf/#Jaynes.filter_step!","page":"Particle filtering","title":"Jaynes.filter_step!","text":"filter_step!(observations::ConstrainedHierarchicalSelection,\n             ps::Particles,\n             new_args::Tuple)\n\nPerform a single filter step from an instance ps of Particles, applying the constraints specified by observations.\n\nfilter_step!(observations::ConstrainedHierarchicalSelection,\n             ps::Particles,\n             new_args::Tuple,\n             proposal::Function,\n             proposal_args::Tuple)\n\nPerform a single filter step using a custom proposal function, applying the constraints specified by observations.\n\n\n\n\n\n","category":"function"},{"location":"inference/pf/","page":"Particle filtering","title":"Particle filtering","text":"info: Info\nCustom proposals provided to filter_step! should accept as first argument a CallSite instance (e.g. so that you can use the previous trace and information in your transition proposal).","category":"page"},{"location":"inference/pf/","page":"Particle filtering","title":"Particle filtering","text":"check_ess_resample!","category":"page"},{"location":"inference/pf/#Jaynes.check_ess_resample!","page":"Particle filtering","title":"Jaynes.check_ess_resample!","text":"check_ess_resample!(ps::Particles)\n\nChecks the effective sample size using ess, then resamples from an existing instance of Particles by mutation in place.\n\n\n\n\n\n","category":"function"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"This page contains a set of benchmarks comparing Jaynes to other probabilistic programming systems. The code for each of these benchmarks is available here.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"warning: Warning\nJaynes is currently in open alpha, which means that these benchmarks should not be taken seriously (any benchmarks between systems must always be taken with a grain of salt anyways) until the system is fully tested.Furthermore, I'm not in the business of inflating the performance characteristics of systems I build. This means:If you notice that I'm not testing Jaynes against optimized programs in other systems, please let me know so that I can perform accurate comparisons.\nIf you suspect that there's an issue or bug with Jaynes, please open an issue.\nIf you'd like to perform a benchmark, also please open an issue.Benchmarking is an inherently sensitive topic - I'd like to make these as fair and open as possible, so don't hesitate to reach out.","category":"page"},{"location":"benchmarks/#Particle-filtering-in-hidden-Markov-models","page":"Benchmarks","title":"Particle filtering in hidden Markov models","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"This benchmark is a single-shot time comparison between Gen and Jaynes on a single thread. The horizontal axis is number of particle filter steps (with resampling). The vertical axis is time in seconds.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"<div style=\"text-align:center\">\n    <img src=\"../images/benchmark_hmmpf_gen_singlethread.png\" alt=\"\" width=\"70%\"/>\n</div>","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"This benchmark is a single-shot time comparison between Gen and Jaynes with adaptive multi-threading in Jaynes.filter_step! and Jaynes.importance_sampling!. This benchmark was executed with JULIA_NUM_THREADS=4.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"<div style=\"text-align:center\">\n    <img src=\"../images/benchmark_hmmpf_gen_multithread.png\" alt=\"\" width=\"70%\"/>\n</div>","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The bottom plots are the estimated log marginal likelihood of the data.","category":"page"},{"location":"library_api/selection_interface/","page":"Selection interface","title":"Selection interface","text":"Jaynes features an extensive Selection query language for addressing sources of randomness. The ability to constrain random choices, compute proposals for random choices in MCMC kernels, as well as gradients requires a solid set of interfaces for selecting addresses in rand calls in your program. Here, we present the main interfaces which you are likely to use. This set of interfaces specifies a sort of query language so you'll find common operations like union, intersection, etc which you can use to flexibly combined selection queries for use in your inference programs and modeling.","category":"page"},{"location":"library_api/selection_interface/#Basic-selections","page":"Selection interface","title":"Basic selections","text":"","category":"section"},{"location":"library_api/selection_interface/","page":"Selection interface","title":"Selection interface","text":"These are the basic set of Selection APIs which allow the user to query and observe throughout the call stack of the program.","category":"page"},{"location":"library_api/selection_interface/","page":"Selection interface","title":"Selection interface","text":"selection\nanywhere","category":"page"},{"location":"library_api/selection_interface/#Jaynes.selection","page":"Selection interface","title":"Jaynes.selection","text":"constraints = selection(a::Vector{Pair{K, J}}) where {K <: Tuple, J}\ntargets = selection(a::Vector{K}) where K <: Tuple\n\nThe first version of the selection API function will produce a ConstrainedHierarchicalSelection with a direct address-based query called ConstrainedByAddress. Here's an example of use:\n\nselection([(:x, ) => 10.0,\n           (:y, :z => 5) => 6.0])\n\nIn the call, the syntax is that addresses at which calls occur are separated by commas. So (:x, ) is just an address in the top-level call and (:y, :z => 5) is an address in the call specified at address :y.\n\nThe second version produces an UnconstrainedHierarchicalSelection with a direct address-based unconstrained query called UnconstrainedByAddress.\n\nselection([(:x, ), (:y, :z)])\n\nThe exact same syntax considerations apply. Unconstrained addressing is used for incremental inference (i.e. methods which use RegenerateContext like Markov chain Monte Carlo).\n\n\n\n\n\n","category":"function"},{"location":"library_api/selection_interface/#Jaynes.anywhere","page":"Selection interface","title":"Jaynes.anywhere","text":"anywhere_selection = anywhere(a::Vector{Pair{K, J}}) where {K <: Tuple, J}\n\nThe anywhere selection API provides access to a special set of constrained and unconstrained selections which will apply at any level of the call stack. Usage of this API requires that you pass in a similar structure to selection - with the caveat that you can only use top-level addresses in the tuple:\n\nanywhere([(:x, ) => 5.0, (:y => 10, ) => 10.0])\n\nThe interpretation: at any place in the call stack where the end of the address is equal to one of the addresses in the selection, the tracer will constrain the address.\n\nThere is also an unconstrained version of anywhere which provides the same interpretation, but for unconstrained targets of RegenerateContext and MCMC algorithms.\n\n\n\n\n\n","category":"function"},{"location":"library_api/selection_interface/#Compositions-of-selections","page":"Selection interface","title":"Compositions of selections","text":"","category":"section"},{"location":"library_api/selection_interface/","page":"Selection interface","title":"Selection interface","text":"The selections produced by the above APIs can be combined compositionally to form more complex selections. Some of these compositions are only available for subtypes of UnconstrainedSelection.","category":"page"},{"location":"library_api/selection_interface/#Selection-utilities","page":"Selection interface","title":"Selection utilities","text":"","category":"section"},{"location":"library_api/selection_interface/","page":"Selection interface","title":"Selection interface","text":"There are a number of useful utility functions defined on subtypes of Selection. Many of these utilities are used internally - here are a few which may be of interest to the user.","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"The majority of the concepts used in the initial implementation of this package come from a combination of research papers and research systems (the most notable in the Julia ecosystem is Gen). See Related Work for a more comprehensive list of references.","category":"page"},{"location":"concepts/#Universal-probabilistic-programming","page":"Concepts","title":"Universal probabilistic programming","text":"","category":"section"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"Probabilistic programming systems are classified according to their ability to express the subset of stochastic computable functions which form valid probability distributions over program execution (in some interpretation). That's a terrible mouthful - but it's wide enough to conveniently capture systems which focus on Bayesian networks, as well as systems which capture a wider set of programs, which we will examine shortly. ","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"Probabilistic programming systems which restrict allowable forms of control flow or recursion are referred to as first-order probabilistic programming systems. The support of the distribution over samples sites which a first-order program defines can be known at compile time - this implies that these programs can be translated safely to a static graph representation (Bayesian network or factor graph) at compile time. This representation can also be attained if control flow can be unrolled using compiler techniques like constant propagation.","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"A static graph representation constructed at compile time is useful, but it's not sufficient to express all valid densities over program execution. Higher-order or universal probabilistic programming frameworks include the ability to handle stochasticity in control flow bounds and recursion. In general, these frameworks include the ability to handle runtime sources of randomness which can't be identified at compile time. To achieve this generality, frameworks which support the ability to express these sorts of probabilistic programs are typically restricted to sampling-based inference methods.","category":"page"},{"location":"concepts/#The-choice-map-abstraction","page":"Concepts","title":"The choice map abstraction","text":"","category":"section"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"One important concept in the universal space is the notion of a mapping from call sites where random choices occur to the values at those sites. This map is called a choice map in most implementations (original representation in Bher). The semantic interpretation of a probabilistic program expressed in a framework which supports universal probabilistic programming via the choice map abstraction is a distribution over choice maps. Consider the following program, which expresses the geometric distribution in this framework:","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"geo(p::Float64) = rand(:flip, Bernoulli, (p, )) == 1 ? 0 : 1 + rand(:geo, geo, p)","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"Here, rand call sites are also given addresses and recursive calls produce a hierarchical address space. A sample from the distribution over choice maps for this program might produce the following map:","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":" :geo => :flip : false\n flip : false\n :geo => (:geo => :flip) : false\n :geo => (:geo => (:geo => :flip)) : false\n :geo => (:geo => (:geo => (:geo => :flip))) : true","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"One simple question arises: what exactly does this distribution over choice maps look like in a mathematical sense? To answer this question, we have to ask how control flow and iteration language features affect the \"abstract space\" of the shape of the program trace. For the moment, we will consider only randomness which occurs explicitly at addresses in each method call (i.e. rand calls with distributions as target) - it turns out that we can safely focus on the shape of the trace in this case without loss of generalization. Randomness which occurs inside of a rand call where the target of the call is another method call can be handled by the same techniques we introduce to analyze the shape of a single method body without target calls.","category":"page"},{"location":"concepts/#Choice-and-call-site-abstractions","page":"Concepts","title":"Choice and call site abstractions","text":"","category":"section"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"Ideally, we'd like the construction of probabilistic programs to parallel the construction of regular programs - we'd like the additional probabilistic semantics to leave the original execution semantics invariant (mostly). In other words, we don't want to give up the powerful abstractions and features which we have become accustomed to while programming in Julia normally. Well, there's good news - you don't have to! You will have to keep a few new things in mind (see the modeling language section for more details) but the whole language should remain open for your use.","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"One of the ways which Jaynes accomplishes this is by creating a set of \"record site\" abstractions which denote places where the tracer can intercept and take over for the normal execution or call semantics which the programmer expects. This notion of an interception site is central to a number of compiler plug-in style systems (IRTools and Cassette included). Systems like these might see a call and intercept the call, possible replacing the call with another call with extra points of overloadability. These systems do this recursively throughout the call stack (neat! ðŸ˜º). As far as I know, it is rare to be able to do this natively in languages. This is a beautiful and deadly part of Julia.","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"To facilitate probabilistic programming, Jaynes intercepts calls to rand (as you might have guessed) and interprets them differently depending on the execution context which the user calls on their toplevel function. The normal Julia execution context is activated by simply calling the toplevel function directly - but Jaynes provides access to a number of additional contexts which perform useful functionality for the design and implementation of sample-based inference algorithms. In general:","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"When Jaynes sees an addressed rand site rand(:x, d) where d is a Distribution instance from the Distributions package, it intercepts it and reasons about it as a ChoiceSite record of the interception, which may include recording some metadata to facilitate inference, or performing other operations.\nWhen Jaynes sees an addressed rand site rand(:x, fn, args...), it intercepts it and reasons about it as a CallSite record of the interception, which may include recording some metadata to facilitate inference, before then recursing into the call to find other points of interception.","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"These are the two basic patterns which are repeated throughout the implementation of execution contexts, which we will see in a moment.","category":"page"},{"location":"concepts/#Implementing-a-context","page":"Concepts","title":"Implementing a context","text":"","category":"section"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"In this section, we'll walk through the implementation of the GenerateContext execution context in full. This should give users of the library a good baseline understanding about how these execution contexts are implemented, and how they do what they do.","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"First, each context is a dynamo - which is a safe IRTools version of Julia's generated functions. Generated functions have access to type information and, thus, method bodies at compile time. Generated functions are typically used to push computation to compile time, but you can do wild things with them. This link showcases a generated function and IR pass which recursively wraps function calls in itself, allow you to use dispatch to intercept function calls and do whatever you want with them at any level of the call stack. This fundamental idea is how libraries like Cassette and the dynamos of IRTools do what they do - this is compiler metaprogramming at its finest (although it is currently hard on the compiler).","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"Dynamos are essentially better behaved generated functions. If you look at that version of Mixtape - it took quite a number of tries to prevent the execution engine from segfaulting out. This never happens with dynamos, with have safe fallbacks when lowered method meta information cannot be acquired. There are also a number of convenient benefits to working with IRTools - the SSA IR format provided by the library is very nice to work with, and there are a number of utilities for compiler enthusiasts to use when writing custom IR passes. For our implementation, we don't need of these advanced utilities - we will start with a simple dynamo:","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"abstract type ExecutionContext end\n\n@dynamo function (mx::ExecutionContext)(a...)\n    ir = IR(a...)\n    ir == nothing && return\n    recur!(ir)\n    return ir\nend","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"This defines a closure - a callable object - as a dynamo (which, remember, is basically an easy-to-work-with generated function). What does this dynamo do? First, it grabs lowered metadata for a particular call, then it converts this metadata to IRTools IR with the call to IR(a...). If the dynamo can't perform this first part of the process, the IR will safely be an instance of Nothing, so the dynamo will return nothing, which means that it just calls the original call with args. If you can derive IR for the call, you pass the IR into recur! which performs the sort of recursive wrapping from Mixtape in a safe way, so that the dynamo wraps every call down the stack (recur! is a Jaynes specific version of recurse! from IRTools which includes a few optimizations specific to Jaynes).","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"The end result of this definition is: if you define any concrete struct which inherits from ExecutionContext, you can call it on a function type and args, and it will wrap itself around every call in the resultant call stack - which means that, as the function call executes, any call on the branch you are on gets wrapped as well, and the transformation repeats itself, until it hits primitives for which it can't derived lowered metadata (and it will just call those primitives, instead of wrapping).","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"What does this afford us? Well, we can now define through dispatch the behavior for any function call we want, for any inheritor of ExecutionContext:","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"mutable struct GenerateContext{T <: Trace, K <: ConstrainedSelection, P <: Parameters} <: ExecutionContext\n    tr::T\n    select::K\n    weight::Float64\n    score::Float64\n    visited::Visitor\n    params::P\nend\n\n@inline function (ctx::GenerateContext)(call::typeof(rand), \n                                        addr::T, \n                                        d::Distribution{K}) where {T <: Address, K}\n    visit!(ctx, addr)\n    if has_query(ctx.select, addr)\n        s = get_query(ctx.select, addr)\n        score = logpdf(d, s)\n        add_choice!(ctx, addr, ChoiceSite(score, s))\n        increment!(ctx, score)\n    else\n        s = rand(d)\n        add_choice!(ctx, addr, ChoiceSite(logpdf(d, s), s))\n    end\n    return s\nend","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"Now, we define a concrete inheritor of ExecutionContext called GenerateContext which keeps a few pieces of metadata around which we will use to record information about calls which include random choices. The inlined closure definition below the struct definition outlines what happens when the dynamo wrapping encounters a call of the following form:","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"rand(addr::T, d::Distribution{K}) where T <: Address","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"where Address is a Union{Symbol, Pair{Symbol, Int}} and is used by the user to denote the sites in their probabilistic program which the tracer will pay attention to. What happens in this call instead of the normal execution for rand(addr, d)? First we do some bookkeeping to make sure the probabilistic program is valid using visit!, then we check a field called select to determine if the user has provided any constraints (i.e. observations) which the execution context should use to constrain this call at this address. If we do have a constraint, we grab the constraint, score it using logpdf for the distribution in the call and add a record of the call to a piece of metadata called a Trace in the execution context. Otherwise, we randomly sample and record the call in the Trace. Finally, we return the sample (or observation) s.","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"This is exactly what happens in the GenerateContext every time the dynamo sees a call of the rand form above instead of the normal execution. But this is exactly what we need to allow sampling of probabilistic programs where some of the address have user-provided constraints. And it all happens automatically, courtesy of compiler metaprogramming.","category":"page"},{"location":"concepts/","page":"Concepts","title":"Concepts","text":"The other execution contexts are implemented in the same way - you'll also notice that this implementation is repeated in the set of specialized call sites which the user can activate if they'd like to express part of a probabilistic program which confirms to a certain structure of randomness dependency. As long as the required interception occurs at the function call level, this compiler metaprogramming technique can be used. Very powerful!","category":"page"},{"location":"architecture/#Implementation","page":"Architecture","title":"Implementation","text":"","category":"section"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"As we started to see in the previous section, Jaynes is organized around a central IRTools dynamo","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"@dynamo function (mx::ExecutionContext)(a...)\n    ir = IR(a...)\n    ir == nothing && return\n    recur!(ir)\n    return ir\nend","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"which defines how instances of inheritors of ExecutionContext act on function calls. The \"wrapping transform\" implemented through recur! is customized, to make the tracer lightweight and to prevent some type stability issues from calls to Base Julia.","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"unwrap(gr::GlobalRef) = gr.name\nunwrap(gr) = gr\n\n# Whitelist includes vectorized calls.\nwhitelist = [:rand, \n             :learnable, \n             :markov, \n             :plate, \n             :ifelse, \n             # Foreign model interfaces\n             :soss_fmi, :gen_fmi, :turing_fmi]\n\n# Fix for specialized tracing.\nfunction recur!(ir, to = self)\n    for (x, st) in ir\n        isexpr(st.expr, :call) && begin\n            ref = unwrap(st.expr.args[1])\n            ref in whitelist || \n            !(unwrap(st.expr.args[1]) in names(Base)) ||\n            continue\n            ir[x] = Expr(:call, to, st.expr.args...)\n        end\n    end\n    return ir\nend\n\n# Fix for _apply_iterate.\nfunction f_push!(arr::Array, t::Tuple{}) end\nf_push!(arr::Array, t::Array) = append!(arr, t)\nf_push!(arr::Array, t::Tuple) = append!(arr, t)\nf_push!(arr, t) = push!(arr, t)\nfunction flatten(t::Tuple)\n    arr = Any[]\n    for sub in t\n        f_push!(arr, sub)\n    end\n    return arr\nend","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"so we see that the tracer is only allowed to look at certain calls, and uses a few fixes for some common issues. This drastically improves the performance over a \"heavyweight\" tracer which looks at everything. For the use case of probabilistic programming implemented in this style, it's perfectly acceptable.","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"There are a number of inheritors for ExecutionContext","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"GenerateContext\nSimulateContext\nProposalContext\nUpdateContext\nRegenerateContext\nScoreContext\nChoiceBackpropagateContext\nParameterBackpropagateContext","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"each of which has a set of special dispatch definition which allows the dynamo to dispatch on rand calls with user-provided addressing. As an example, here's the interception dispatch inside the GenerateContext (which we just examined in the last section):","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"@inline function (ctx::GenerateContext)(call::typeof(rand), \n                                        addr::T, \n                                        d::Distribution{K}) where {T <: Address, K}\n    visit!(ctx, addr)\n    if has_query(ctx.select, addr)\n        s = get_query(ctx.select, addr)\n        score = logpdf(d, s)\n        add_choice!(ctx, addr, ChoiceSite(score, s))\n        increment!(ctx, score)\n    else\n        s = rand(d)\n        add_choice!(ctx, addr, ChoiceSite(logpdf(d, s), s))\n    end\n    return s\nend\n","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"so this context records the random choice, as well as performs some bookkeeping which we use for inference. Each of the other contexts define unique interception dispatch to implement functionality required for inference over probabilistic program traces. These can be found here.","category":"page"},{"location":"architecture/#Record-sites-and-traces","page":"Architecture","title":"Record sites and traces","text":"","category":"section"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"The conceptual entities of the Jaynes tracing system are record sites and traces.","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"abstract type RecordSite end\nabstract type Trace end","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"which, respectively, represent sites at which randomness occurs (and is traced) and the trace itself. Let's examine one type of site, a ChoiceSite:","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"struct ChoiceSite{T} <: RecordSite\n    score::Float64\n    val::T\nend","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"A ChoiceSite is just a record of a random selection, along with the log probability of the selection with respect to the user-specified distribution at that site. These are created by calls of the form rand(addr::Address, d::Distribution) where Distribution is the type from the Distributions library.","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"The structured representation of recorded randomness in a program execution is a Trace:","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"struct HierarchicalTrace <: Trace\n    calls::Dict{Address, CallSite}\n    choices::Dict{Address, ChoiceSite}\n    params::Dict{Address, LearnableSite}\nend","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"which has separate fields for call sites and choice sites:","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"Here, I'm showing HierarchicalTrace which is used in black-box calls as the default trace. Here's VectorizedTrace which is activated by special language calls (for now, markov and plate, likely more in the future):","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"struct VectorizedTrace{C <: RecordSite} <: Trace\n    subrecords::Vector{C}\n    params::Dict{Address, LearnableSite}\nend","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"This trace explicitly represents certain dependency information in the set of calls specified by the language calls - e.g. markov specifies a Markovian dependency from one call to the next and plate specifies IID calls.","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"We just encountered ChoiceSite above in GenerateContext - let's look at an example CallSite:","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"struct HierarchicalCallSite{J, K} <: CallSite\n    trace::HierarchicalTrace\n    score::Float64\n    fn::Function\n    args::J\n    ret::K\nend","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"This call site is how we represent black box function calls which the user has indicated need to be traced. Other call sites present unique functionality, which (when traced) provide the contexts used for inference with additional information which can speed up certain operations.","category":"page"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"Generically, these entities are all that are required to construct a set of inference APIs over program traces and the choice maps represented in those traces. Other advanced functionality (like specialized call sites) are variations on these themes.","category":"page"},{"location":"modeling_lang/","page":"Modeling language","title":"Modeling language","text":"The modeling language for Jaynes is ... Julia! We don't require the use of macros to specify probabilistic models, because the tracer tracks code using introspection at the level of lowered (and IR) code.","category":"page"},{"location":"modeling_lang/","page":"Modeling language","title":"Modeling language","text":"function bayeslinreg(N::Int)\n    Ïƒ = rand(:Ïƒ, InverseGamma(2, 3))\n    Î² = rand(:Î², Normal(0.0, 1.0))\n    y = Vector{Float64}(undef, N)\n    for i in 1:N\n        y[i] = rand(:y => x, Normal(Î²*x, Ïƒ))\n    end\n    return y\nend","category":"page"},{"location":"modeling_lang/","page":"Modeling language","title":"Modeling language","text":"However, this doesn't give you free reign to write anything with rand calls and expect it to compile to a valid probabilistic program. Here we outline a number of restrictions (originally formalized in Gen) which are required to allow inference to stay strictly Bayesian.","category":"page"},{"location":"modeling_lang/","page":"Modeling language","title":"Modeling language","text":"For branches with address spaces which intersect, the addresses in the intersection must have distributions with the same base measure. This means you cannot swap continuous for discrete or vice versa depending on which branch you're on.\nMutable state does not interact well with iterative inference (e.g. MCMC). Additionally, be careful about the support of your distributions in this regard. If you're going to use mutable state in your programs, use rand calls in a lightweight manner - only condition on distributions with constant support and be careful about MCMC.","category":"page"},{"location":"modeling_lang/#Specialized-call-sites","page":"Modeling language","title":"Specialized call sites","text":"","category":"section"},{"location":"modeling_lang/","page":"Modeling language","title":"Modeling language","text":"Jaynes also offers a set of primitive language features for creating specialized call sites which are similar to the combinators of Gen. These special features can be activated by a special set of calls (below, markov and plate).","category":"page"},{"location":"modeling_lang/","page":"Modeling language","title":"Modeling language","text":"using Jaynes\n\nfunction bar(m, n)\n    x = rand(:x, Normal(m, 1.0))\n    q = rand(:q, Normal(x, 5.0))\n    z = rand(:z, Normal(n + q, 3.0))\n    return q, z\nend\n\nfunction foo()\n    x = markov(:x, bar, 10, 0.3, 3.0)\n    y = plate(:y, bar, x)\n    return y\nend\n\nret, cl = simulate(foo)\ndisplay(cl.trace; show_values = true)","category":"page"},{"location":"modeling_lang/","page":"Modeling language","title":"Modeling language","text":"Here, the markov and plate calls provide explicit knowledge to the tracer that the generation of randomness conforms to a computation pattern which can be vectorized. This allows the tracer to construct an efficient VectorizedCallSite which allows more efficient updates/regenerations than a \"black-box\" CallSite where the dependency information may not be known. This is a simple way for the user to increase the efficiency of inference algorithms, by informing the tracer of information which it can't derive on its own (at least for now ðŸ˜º).","category":"page"},{"location":"modeling_lang/","page":"Modeling language","title":"Modeling language","text":"markov requires that the user provide a function f with","category":"page"},{"location":"modeling_lang/","page":"Modeling language","title":"Modeling language","text":"f (X Y) rightarrow (X Y)","category":"page"},{"location":"modeling_lang/","page":"Modeling language","title":"Modeling language","text":"as well as a first argument which denotes the number of fold operations to compute (in the example above, 10). markov will then iteratively compute the function, passing the return value as arguments to the next computation (from left to right).","category":"page"},{"location":"modeling_lang/","page":"Modeling language","title":"Modeling language","text":"plate does not place requirements on the function f (other than the implicit requirements for valid programs, as described above) but does require that the arguments be a Vector with each element matching the signature of f. plate then iteratively applies the function as a kernel for each element in the argument vector (similar to the functional map operation).","category":"page"},{"location":"modeling_lang/","page":"Modeling language","title":"Modeling language","text":"In the future, a number of other specialized call sites are planned. The fallback is always black-box tracing, but if you provide the tracer with more information about the dependency structure of your probabilistic program, it can utilize this to accelerate iterative inference algorithms. One interesting research direction is the automatic discovery of these patterns in programs: if you're interested in this, please contribute to the open issue about automatic structure discovery.","category":"page"},{"location":"inference/is/","page":"Importance sampling","title":"Importance sampling","text":"CurrentModule = Jaynes","category":"page"},{"location":"inference/is/","page":"Importance sampling","title":"Importance sampling","text":"importance_sampling","category":"page"},{"location":"inference/is/#Jaynes.importance_sampling","page":"Importance sampling","title":"Jaynes.importance_sampling","text":"Samples from the model prior.\n\nparticles, normalized_weights = importance_sampling(observations::ConstrainedSelection,\n                                                    num_samples::Int,\n                                                    model::Function, \n                                                    args::Tuple)\n\nSamples from a programmer-provided proposal function.\n\nparticles, normalized_weights = importance_sampling(observations::ConstrainedSelection,\n                                                    num_samples::Int,\n                                                    model::Function, \n                                                    args::Tuple, \n                                                    proposal::Function, \n                                                    proposal_args::Tuple)\n\nRun importance sampling on the posterior over unconstrained addresses and values. Returns an instance of Particles and normalized weights.\n\n\n\n\n\n","category":"function"},{"location":"inference/is/","page":"Importance sampling","title":"Importance sampling","text":"info: Info\nAddressed randomness in a custom proposal passed to importance_sampling should satisfy the following criteria to ensure that inference is mathematically valid:Custom proposals should only propose to unobserved addresses in the original program.\nCustom proposals should not propose to addresses which do not occur in the original program.","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"CurrentModule = Jaynes","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"Jaynes supports Zygote-based reverse mode gradient computation of learnable parameters and primitive probabilistic choices. This functionality is accessed through two different gradient contexts.","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"ParameterBackpropagateContext","category":"page"},{"location":"library_api/diff_prog/#Jaynes.ParameterBackpropagateContext","page":"Differentiable programming","title":"Jaynes.ParameterBackpropagateContext","text":"mutable struct ParameterBackpropagateContext{T <: Trace} <: BackpropagationContext\n    tr::T\n    weight::Float64\n    visited::Visitor\n    initial_params::Parameters\n    params::ParameterStore\n    param_grads::Gradients\nend\n\nParameterBackpropagateContext is used to compute the gradients of parameters with respect to following objective:\n\nOuter constructors:\n\nParameterBackpropagate(tr::T, params) where T <: Trace = ParameterBackpropagateContext(tr, 0.0, Visitor(), params, Gradients())\nParameterBackpropagate(tr::T, params, param_grads::Gradients) where {T <: Trace, K <: UnconstrainedSelection} = ParameterBackpropagateContext(tr, 0.0, Visitor(), params, param_grads)\n\n\n\n\n\n","category":"type"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"For one-shot gradient computations, this context is easily accessed through the get_parameter_gradients method.","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"get_parameter_gradients","category":"page"},{"location":"library_api/diff_prog/#Jaynes.get_parameter_gradients","page":"Differentiable programming","title":"Jaynes.get_parameter_gradients","text":"gradients = get_parameter_gradients(params, cl::T, ret_grad, scaler::Float64 = 1.0) where T <: CallSite\n\nReturns a Gradients object which tracks the gradients of the objective with respect to parameters in the program.\n\n\n\n\n\n","category":"function"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"For choices, the context is ChoiceBackpropagateContext.","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"ChoiceBackpropagateContext","category":"page"},{"location":"library_api/diff_prog/#Jaynes.ChoiceBackpropagateContext","page":"Differentiable programming","title":"Jaynes.ChoiceBackpropagateContext","text":"mutable struct ChoiceBackpropagateContext{T <: Trace} <: BackpropagationContext\n    tr::T\n    weight::Float64\n    visited::Visitor\n    initial_params::Parameters\n    params::ParameterStore\n    param_grads::Gradients\nend\n\nChoiceBackpropagateContext is used to compute the gradients of choices with respect to following objective:\n\nOuter constructors:\n\nChoiceBackpropagate(tr::T, init_params, params, choice_grads) where {T <: Trace, K <: UnconstrainedSelection} = ChoiceBackpropagateContext(tr, 0.0, Visitor(), params, choice_grads, UnconstrainedAllSelection())\nChoiceBackpropagate(tr::T, init_params, params, choice_grads, sel::K) where {T <: Trace, K <: UnconstrainedSelection} = ChoiceBackpropagateContext(tr, 0.0, Visitor(), params, choice_grads, sel)\n\n\n\n\n\n","category":"type"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"For one-shot gradient computations on choices, the ChoiceBackpropagateContext is easily accessed through the get_choice_gradients method.","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"get_choice_gradients","category":"page"},{"location":"library_api/diff_prog/#Jaynes.get_choice_gradients","page":"Differentiable programming","title":"Jaynes.get_choice_gradients","text":"gradients = get_choice_gradients(params, cl::T, ret_grad) where T <: CallSite\ngradients = get_choice_gradients(cl::T, ret_grad) where T <: CallSite\n\nReturns a Gradients object which tracks the gradients with respect to the objective of random choices with differentiable logpdf in the program.\n\n\n\n\n\n","category":"function"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"In the future, Jaynes will support a context which allows the automatic training of neural network components (Flux.jl or otherwise) facilitated by custom call sites. See the foreign model interface for more details.","category":"page"},{"location":"library_api/diff_prog/#Updating-parameters","page":"Differentiable programming","title":"Updating parameters","text":"","category":"section"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"As part of standard usage, it's likely that you'd like to update learnable parameters in your model (which you declare with learnable(addr, initial_value). ","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"warning: Warning\nCurrently, there's a bug with declaring learnable structures which prevents the usage of parameters of type other than Float64 or Array{Float64, 1}. If you run into Zygote errors with mutating arrays, you can try to alleviate the problem by annotating your parameters (e.g. Float64[1.0, 3.0, ...]). You'll mostly be okay if you can stick to scalar parameters and 1D arrays - I'm working to identify this issue and fix it so higher-rank tensors can also be used.","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"Given a CallSite, you can extract parameters using get_parameters","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"ret, cl, w = generate(selection((:q, -0.5)), learnable_hypers)\nparams = get_parameters(cl)","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"which produces an instance of LearnableParameters by unpacking the params field in any trace kept in the call site. LearnableParameters and Gradients are explicitly kept separate from the internal call site representation, to encourage portability, as well as non-standard optimization schemes. Jaynes links up with the API provided by Flux.Optimisers through update! - this allows you to use any of the optimisers provided by Flux to update your parameters. Given a Gradients instance, to update your parameters, just call update_parameters with your favorite optimiser","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"trained_params = update_parameters(opt, params, gradients)","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"which produces a new instance of LearnableParameters after apply the gradient descent step. You can then pass these in as an argument to the normal context interfaces (e.g. generate, simulate, etc) to use your updated parameters.","category":"page"},{"location":"related_work/#Probabilistic-programming","page":"Related work","title":"Probabilistic programming","text":"","category":"section"},{"location":"related_work/","page":"Related work","title":"Related work","text":"There are a number of good references in both textbook and research paper form on probabilistic programming. In reference to this system, here are a few I would recommend:","category":"page"},{"location":"related_work/","page":"Related work","title":"Related work","text":"An Introduction to Probabilistic Programming\nA compilation target for probabilistic programming languages\nLightweight implementations of probabilistic programming languages via transformational compilation\nProbabilistic programming\nThe design and implementation of probabilistic programming languages\nThe principles and practice of probabilistic programming","category":"page"},{"location":"related_work/","page":"Related work","title":"Related work","text":"There are also numerous papers about current systems:","category":"page"},{"location":"related_work/","page":"Related work","title":"Related work","text":"Figaro: an object-oriented probabilistic programming language\nTuring: a language for flexible probabilistic inference\nVenture: a higher-order probabilistic programming platform with programmable inference\nProbabilistic inference by program transformation in Hakaru\nGen: a general-purpose probabilistic programming system with programmable inference\nFACTORIE: probabilistic programming via imperatively defined factor graphs\n(monad-bayes) Practical probabilistic programming with monads\n(probabilistic C) A compilation target for probabilistic programming languages","category":"page"},{"location":"related_work/#Implementation-and-design","page":"Related work","title":"Implementation and design","text":"","category":"section"},{"location":"related_work/","page":"Related work","title":"Related work","text":"Jaynes is a context-oriented programming system for probabilistic programming. Internally, the current implementation closely follows the design of the dynamic DSL in Gen which also uses the notion of stateful execution contexts to produce the interfaces required for inference. Jaynes is focused on an optimized dynamic language which allows most of the Julia language to be used in expressing probabilistic programs.","category":"page"},{"location":"related_work/","page":"Related work","title":"Related work","text":"I would recommend users of Jaynes become familiar with Gen - to understand the problems which Jaynes attempts to solve. The following papers may be useful in this regard:","category":"page"},{"location":"related_work/","page":"Related work","title":"Related work","text":"Gen: a general-purpose probabilistic programming system with programmable inference\nProbabilistic programming with programmable inference\nA new approach to probabilistic programming inference\nLightweight implementations of probabilistic programming languages via transformational compilation","category":"page"},{"location":"related_work/","page":"Related work","title":"Related work","text":"In the design space of compiler metaprogramming tools, the following systems have been highly influential in the design of Jaynes","category":"page"},{"location":"related_work/","page":"Related work","title":"Related work","text":"IRTools\nCassette","category":"page"},{"location":"related_work/","page":"Related work","title":"Related work","text":"In particular, IRTools provides a large part of the core infrastructure for the implementation. Strictly speaking, Jaynes is not dependent on a fundamental mechanism which only IRTools provides (anything can be expressed with generated functions from Julia) but IRTools greatly reduces the level of risk in working with generated functions and lowered code.","category":"page"},{"location":"related_work/","page":"Related work","title":"Related work","text":"Jaynes has also been influenced by Turing, the Poutine effects system in Pyro, and Unison lang. Jaynes does not implement algebraic effects in a rigorous (or static!) way, but the usage of execution contexts which control how certain method calls are executed is closely aligned with these concepts.","category":"page"},{"location":"related_work/","page":"Related work","title":"Related work","text":"Finally, the probabilistic programming community in Julia is largely responsible for many of the ideas and conversations which lead to Jaynes. I'd like to thank Chad Scherrer, Martin Trapp, Alex Lew, Jarred Barber, George Matheos, Marco Cusumano-Towner, Ari Katz, Philipp Gabler, Valentin Churavy, Mike Innes, and Lyndon White for auxiliary help and discussion concerning the design and implementation of many parts of the system.","category":"page"},{"location":"inference/vi/","page":"Automatic differentiation variational inference","title":"Automatic differentiation variational inference","text":"CurrentModule = Jaynes","category":"page"},{"location":"inference/vi/","page":"Automatic differentiation variational inference","title":"Automatic differentiation variational inference","text":"advi","category":"page"},{"location":"inference/vi/#Jaynes.advi","page":"Automatic differentiation variational inference","title":"Jaynes.advi","text":"params, elbows, call_sites =  advi(sel::K,\n                                   iters::Int,\n                                   v_mod::Function,\n                                   v_args::Tuple,\n                                   mod::Function,\n                                   args::Tuple;\n                                   opt = ADAM(),\n                                   gs_samples = 100) where K <: ConstrainedSelection\n\nGiven a selection sel, perform automatic-differentiation variational inference with a proposal model v_mod. The result is a new set of trained parameters params for the variational model, the history of ELBO estimates elbows, and the call sites calls produced by the gradient estimator computation.\n\n\n\n\n\n","category":"function"},{"location":"library_api/contexts/","page":"Execution contexts","title":"Execution contexts","text":"CurrentModule = Jaynes","category":"page"},{"location":"library_api/contexts/","page":"Execution contexts","title":"Execution contexts","text":"Here is a description of the set of \"standard\" contexts which are used frequently in modeling and inference. Gradient and foreign model contexts are discussed in Differentiable programming and Foreign model interface, respectively.","category":"page"},{"location":"library_api/contexts/","page":"Execution contexts","title":"Execution contexts","text":"GenerateContext\ngenerate","category":"page"},{"location":"library_api/contexts/#Jaynes.GenerateContext","page":"Execution contexts","title":"Jaynes.GenerateContext","text":"mutable struct GenerateContext{T <: Trace, K <: ConstrainedSelection, P <: Parameters} <: ExecutionContext\n     tr::T\n     select::K\n     weight::Float64\n     score::Float64\n     visited::Visitor\n     params::P\nend\n\nGenerateContext is used to generate traces, as well as record and accumulate likelihood weights given observations at addressed randomness.\n\nInner constructors:\n\nGenerateContext(tr::T, select::K) where {T <: Trace, K <: ConstrainedSelection} = new{T, K}(tr, select, 0.0, Visitor(), Parameters())\nGenerateContext(tr::T, select::K, params::P) where {T <: Trace, K <: ConstrainedSelection, P <: Parameters} = new{T, K, P}(tr, select, 0.0, Visitor(), params)\n\nOuter constructors:\n\nGenerate(select::ConstrainedSelection) = GenerateContext(Trace(), select)\nGenerate(select::ConstrainedSelection, params) = GenerateContext(Trace(), select, params)\nGenerate(tr::Trace, select::ConstrainedSelection) = GenerateContext(tr, select)\n\n\n\n\n\n","category":"type"},{"location":"library_api/contexts/#Jaynes.generate","page":"Execution contexts","title":"Jaynes.generate","text":"ret, cl, w = generate(sel::L, fn::Function, args...; params = Parameters()) where L <: ConstrainedSelection\nret, cs, w = generate(sel::L, fn::typeof(rand), d::Distribution{K}; params = Parameters()) where {L <: ConstrainedSelection, K}\nret, v_cl, w = generate(sel::L, fn::typeof(markov), call::Function, len::Int, args...; params = Parameters()) where L <: ConstrainedSelection\nret, v_cl, w = generate(sel::L, fn::typeof(plate), call::Function, args::Vector; params = Parameters()) where L <: ConstrainedSelection\nret, v_cl, w = generate(sel::L, fn::typeof(plate), d::Distribution{K}, len::Int; params = Parameters()) where {L <: ConstrainedSelection, K}\n\ngenerate provides an API to the GenerateContext execution context. You can use this function on any of the matching signatures above - it will return the return value ret, a RecordSite instance specialized to the call, and the score/weight w computed with respect to the constraints sel.\n\n\n\n\n\n","category":"function"},{"location":"library_api/contexts/","page":"Execution contexts","title":"Execution contexts","text":"SimulateContext\nsimulate","category":"page"},{"location":"library_api/contexts/#Jaynes.SimulateContext","page":"Execution contexts","title":"Jaynes.SimulateContext","text":"mutable struct SimulateContext{T <: Trace} <: ExecutionContext\n    tr::T\n    visited::Visitor\n    params::LearnableParameters\n    SimulateContext(params) where T <: Trace = new{T}(Trace(), Visitor(), params)\nend\n\nSimulateContext is used to simulate traces without recording likelihood weights. SimulateContext can be instantiated with custom LearnableParameters instances, which is useful when used for gradient-based learning.\n\nInner constructors:\n\nSimulateContext(params) = new{HierarchicalTrace}(Trace(), Visitor(), params)\n\n\n\n\n\n","category":"type"},{"location":"library_api/contexts/#Jaynes.simulate","page":"Execution contexts","title":"Jaynes.simulate","text":"ret, cl = simulate(fn::Function, args...; params = LearnableParameters())\nret, cl = simulate(fn::typeof(rand), d::Distribution{T}; params = LearnableParameters()) where T\nret, v_cl = simulate(c::typeof(plate), fn::Function, args::Vector; params = LearnableParameters()) where T\nret, v_cl = simulate(fn::typeof(plate), d::Distribution{T}, len::Int; params = LearnableParameters()) where T\nret, v_cl = simulate(c::typeof(markov), fn::Function, len::Int, args...; params = LearnableParameters())\n\nsimulate function provides an API to the SimulateContext execution context. You can use this function on any of the matching signatures above - it will return the return value ret, and a RecordSite instance specialized to the call. simulate is used to express unconstrained generation of a probabilistic program trace, without likelihood weight recording.\n\n\n\n\n\n","category":"function"},{"location":"library_api/contexts/","page":"Execution contexts","title":"Execution contexts","text":"ProposeContext\npropose","category":"page"},{"location":"library_api/contexts/#Jaynes.ProposeContext","page":"Execution contexts","title":"Jaynes.ProposeContext","text":"mutable struct ProposeContext{T <: Trace} <: ExecutionContext\n    tr::T\n    score::Float64\n    visited::Visitor\n    params::LearnableParameters\nend\n\nProposeContext is used to propose traces for inference algorithms which use custom proposals. ProposeContext instances can be passed sets of LearnableParameters to configure the propose with parameters which have been learned by differentiable programming.\n\nInner constructors:\n\nProposeContext(tr::T) where T <: Trace = new{T}(tr, 0.0, LearnableParameters())\n\nOuter constructors:\n\nPropose() = ProposeContext(Trace())\n\n\n\n\n\n","category":"type"},{"location":"library_api/contexts/#Jaynes.propose","page":"Execution contexts","title":"Jaynes.propose","text":"ret, g_cl, w = propose(fn::Function, args...)\nret, cs, w = propose(fn::typeof(rand), d::Distribution{K}) where K\nret, v_cl, w = propose(fn::typeof(markov), call::Function, len::Int, args...)\nret, v_cl, w = propose(fn::typeof(plate), call::Function, args::Vector)\nret, v_cl, w = propose(fn::typeof(plate), d::Distribution{K}, len::Int) where K\n\npropose provides an API to the ProposeContext execution context. You can use this function on any of the matching signatures above - it will return the return value ret, a RecordSite instance specialized to the call, and the score w.\n\n\n\n\n\n","category":"function"},{"location":"library_api/contexts/","page":"Execution contexts","title":"Execution contexts","text":"ScoreContext\nscore","category":"page"},{"location":"library_api/contexts/#Jaynes.ScoreContext","page":"Execution contexts","title":"Jaynes.ScoreContext","text":"mutable struct ScoreContext{P <: Parameters} <: ExecutionContext\n    select::ConstrainedSelection\n    weight::Float64\n    params::P\nend\n\nThe ScoreContext is used to score selections according to a model function. For computation in the ScoreContext to execute successfully, the select selection must provide a choice for every address visited in the model function, and the model function must allow the context to visit every constraints expressed in select.\n\nInner constructors:\n\nfunction Score(obs::Vector{Tuple{K, P}}) where {P, K <: Union{Symbol, Pair}}\n    c_sel = selection(obs)\n    new{EmptyParameters}(c_sel, 0.0, Parameters())\nend\n\nOuter constructors:\n\nScoreContext(obs::K, params) where {K <: ConstrainedSelection} = new(obs, 0.0, params)\nScore(obs::Vector) = ScoreContext(selection(obs))\nScore(obs::ConstrainedSelection) = ScoreContext(obs, Parameters())\nScore(obs::ConstrainedSelection, params) = ScoreContext(obs, params)\n\n\n\n\n\n","category":"type"},{"location":"library_api/contexts/#Jaynes.score","page":"Execution contexts","title":"Jaynes.score","text":"ret, w = score(sel::L, fn::Function, args...; params = Parameters()) where L <: ConstrainedSelection\nret, w = score(sel::L, fn::typeof(rand), d::Distribution{K}; params = Parameters()) where {L <: ConstrainedSelection, K}\nret, w = score(sel::L, fn::typeof(markov), call::Function, len::Int, args...; params = Parameters()) where L <: ConstrainedSelection\nret, w = score(sel::L, fn::typeof(plate), call::Function, args::Vector; params = Parameters()) where L <: ConstrainedSelection\nret, w = score(sel::L, fn::typeof(plate), d::Distribution{K}, len::Int; params = Parameters()) where {L <: ConstrainedSelection, K}\n\nscore provides an API to the ScoreContext execution context. You can use this function on any of the matching signatures above - it will return the return value ret, and the likelihood weight score of the user-provided selection sel. The selection should satisfy the following requirement:\n\nAt any random choice in any branch traveled according to the constraints of sel, sel must provide a constraint for that choice.\n\nSimply put, this just means you need to provide a constraint for each ChoiceSite you encounter.\n\n\n\n\n\n","category":"function"},{"location":"library_api/contexts/","page":"Execution contexts","title":"Execution contexts","text":"UpdateContext\nupdate","category":"page"},{"location":"library_api/contexts/#Jaynes.UpdateContext","page":"Execution contexts","title":"Jaynes.UpdateContext","text":"mutable struct UpdateContext{C <: CallSite, \n                             T <: Trace,\n                             K <: ConstrainedSelection, \n                             P <: Parameters, \n                             D <: Diff} <: ExecutionContext\n    prev::C\n    tr::T\n    select::K\n    weight::Float64\n    score::Float64\n    discard::HierarchicalTrace\n    visited::Visitor\n    params::P\n    argdiffs::D\nend\n\nInner constructor:\n\nUpdateContext(cl::C, select::K, argdiffs::D) where {C <: CallSite, K <: ConstrainedSelection, D <: Diff} = new{C, typeof(cl.trace), K, EmptyParameters, D}(cl, typeof(cl.trace)(), select, 0.0, 0.0, Trace(), Visitor(), Parameters(), argdiffs)\n\nUpdateContext is an execution context used for updating the value of random choices in an existing recorded call site. This context will perform corrective updates to the likehood weights and scores so that this operation produces the correct weights and scores for the original model program constrained with the select selection in the UpdateContext.\n\n\n\n\n\n","category":"type"},{"location":"library_api/contexts/#Jaynes.update","page":"Execution contexts","title":"Jaynes.update","text":"ret, cl, w, retdiff, d = update(ctx::UpdateContext, bbcs::HierarchicalCallSite, args...) where D <: Diff\nret, cl, w, retdiff, d = update(sel::L, bbcs::HierarchicalCallSite) where L <: ConstrainedSelection\nret, cl, w, retdiff, d = update(sel::L, bbcs::HierarchicalCallSite, argdiffs::D, new_args...) where {L <: ConstrainedSelection, D <: Diff}\nret, v_cl, w, retdiff, d = update(sel::L, vcs::VectorizedCallSite{typeof(plate)}) where {L <: ConstrainedSelection, D <: Diff}\nret, v_cl, w, retdiff, d = update(sel::L, vcs::VectorizedCallSite{typeof(markov)}) where {L <: ConstrainedSelection, D <: Diff}\nret, v_cl, w, retdiff, d = update(sel::L, vcs::VectorizedCallSite{typeof(markov)}, d::NoChange, len::Int) where {L <: ConstrainedSelection, D <: Diff}\nret, v_cl, w, retdiff, d = update(sel::L, vcs::VectorizedCallSite{typeof(markov)}, len::Int) where {L <: ConstrainedSelection, D <: Diff}\n\nupdate provides an API to the UpdateContext execution context. You can use this function on any of the matching signatures above - it will return the return value ret, the updated RecordSite instance cl or v_cl, the updated weight w, a Diff instance for the return value retdiff, and a structure which contains any changed (i.e. discarded) record sites d.\n\n\n\n\n\n","category":"function"},{"location":"library_api/contexts/","page":"Execution contexts","title":"Execution contexts","text":"RegenerateContext\nregenerate","category":"page"},{"location":"library_api/contexts/#Jaynes.RegenerateContext","page":"Execution contexts","title":"Jaynes.RegenerateContext","text":"mutable struct RegenerateContext{T <: Trace, \n                                 L <: UnconstrainedSelection,\n                                 P <: Parameters} <: ExecutionContext\n    prev::T\n    tr::T\n    select::L\n    weight::Float64\n    score::Float64\n    discard::T\n    visited::Visitor\n    params::P\nend\n\nInner constructors:\n\nfunction RegenerateContext(tr::T, sel::Vector{Address}) where T <: Trace\n    un_sel = selection(sel)\n    new{T, typeof(un_sel), EmptyParameters}(tr, Trace(), un_sel, 0.0, Trace(), Visitor(), Parameters())\nend\nfunction RegenerateContext(tr::T, sel::L) where {T <: Trace, L <: UnconstrainedSelection}\n    new{T, L, EmptyParameters}(tr, Trace(), sel, 0.0, Trace(), Visitor(), Parameters())\nend\n\nOuter constructors:\n\nRegenerate(tr::Trace, sel::Vector{Address}) = RegenerateContext(tr, sel)\nRegenerate(tr::Trace, sel::UnconstrainedSelection) = RegenerateContext(tr, sel)\n\nThe RegenerateContext is used for MCMC algorithms, to propose new choices for addresses indicated by an UnconstrainedSelection in the select field.\n\n\n\n\n\n","category":"type"},{"location":"library_api/contexts/#Jaynes.regenerate","page":"Execution contexts","title":"Jaynes.regenerate","text":"ret, cl = regenerate(sel::L, bbcs::HierarchicalCallSite, new_args...) where L <: UnconstrainedSelection\nret, cl = regenerate(sel::L, bbcs::HierarchicalCallSite) where L <: UnconstrainedSelection\n\nregenerate is an API to the RegenerateContext execution context. regenerate requires that users provide an UnconstrainedSelection, an original call site, and possibly a set of new arguments to be used in the regeneration step. This context internally keeps track of the bookkeeping required to increment likelihood weights, as well as prune off parts of the trace which are invalid if a regenerated choice changes the shape of the trace (e.g. control flow), and returns a new return value ret as well as the modified call site cl.\n\n\n\n\n\n","category":"function"},{"location":"examples/","page":"Examples","title":"Examples","text":"This page keeps a set of small examples expressed in Jaynes.","category":"page"},{"location":"examples/#Geometric-program","page":"Examples","title":"Geometric program","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"module Geometric\n\nusing Jaynes\nusing Distributions\n\ngeo(p::Float64) = rand(:flip, Bernoulli(p)) ? 1 : 1 + rand(:geo, geo, p)\n\nret, cl = simulate(geo, 0.2)\ndisplay(cl.trace)\n\nend # module","category":"page"},{"location":"examples/#Bayesian-linear-regression","page":"Examples","title":"Bayesian linear regression","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This example uses the plate special language feature (equivalent to plate notation) to perform IID draws from the Normal specified in the program. This is more concise than a loop, and also allows inference algorithms to utilize efficient operations for updating and regenerating choices.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"module BayesianLinearRegression\n\nusing Jaynes\nusing Distributions\n\nfunction bayesian_linear_regression(N::Int)\n    Ïƒ = rand(:Ïƒ, InverseGamma(2, 3))\n    Î² = rand(:Î², Normal(0.0, 1.0))\n    y = plate(:y, x -> rand(:draw, Normal(Î²*x, Ïƒ)), [Float64(i) for i in 1 : N])\nend\n\n# Example trace.\nret, cl = simulate(bayesian_linear_regression, 10)\ndisplay(cl.trace)\n\nsel = selection(map(1 : 100) do i\n                    (:y => i => :draw, Float64(i) + rand())\n                end)\n\n@time ps, ret = importance_sampling(sel, 5000, bayesian_linear_regression, (100, ))\n\nnum_res = 1000\nresample!(ps, num_res)\nmean_Ïƒ = sum(map(ps.calls) do cl\n                 cl[:Ïƒ]\n             end) / num_res\nprintln(\"Mean Ïƒ: $mean_Ïƒ\")\n\nmean_Î² = sum(map(ps.calls) do cl\n                 cl[:Î²]\n             end) / num_res\nprintln(\"Mean Î²: $mean_Î²\")\n\nend # module","category":"page"},{"location":"examples/#Backpropagation-for-choices-and-learnable-parameters","page":"Examples","title":"Backpropagation for choices and learnable parameters","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"module Learnable\n\nusing Jaynes\nusing Distributions\n\nfunction foo(q::Float64)\n    p = learnable(:l, 10.0)\n    z = rand(:z, Normal(p, q))\n    return z\nend\n\nfunction bar(x::Float64, y::Float64)\n    l = learnable(:l, 5.0)\n    m = learnable(:m, 10.0)\n    q = rand(:q, Normal(l, y + m))\n    f = rand(:f, foo, 5.0)\n    return f\nend\n\ncl = trace(bar, 5.0, 1.0)\n\nparams = get_parameters(cl)\nprintln(\"Parameters:\\n$(params)\")\n\ngrads = get_parameter_gradients(cl, 1.0)\nprintln(\"\\nParameter gradients:\\n$(grads)\")\n\ngrads = get_choice_gradients(cl, 1.0)\nprintln(\"\\nChoice gradients:\\n$(grads)\")\n\nend # module","category":"page"},{"location":"examples/#Specialized-Markov-call-sites","page":"Examples","title":"Specialized Markov call sites","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This example illustrates how specialized call sites (accessed through the tracer language features) can be used to accelerate inference.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"module HiddenMarkovModel\n\nusing Jaynes\nusing Distributions\n\nfunction kernel(prev_latent::Float64)\n    z = rand(:z, Normal(prev_latent, 1.0))\n    x = rand(:x, Normal(z, 3.0))\n    return z\nend\n\n# Specialized Markov call site informs tracer of dependency information.\nkernelize = n -> markov(:k, kernel, n, 5.0)\n\nsimulation = () -> begin\n    ps = initialize_filter(selection(), 5000, kernelize, (1, ))\n    for i in 2:50\n        sel = selection((:k => i => :x, 5.0))\n\n        # Complexity of filter step is constant as a size of the trace.\n        @time filter_step!(sel, ps, NoChange(), (i,))\n    end\n    return ps\nend\n\nps = simulation()\n\nend # module","category":"page"},{"location":"inference/mh/","page":"Metropolis-Hastings","title":"Metropolis-Hastings","text":"CurrentModule = Jaynes","category":"page"},{"location":"inference/mh/","page":"Metropolis-Hastings","title":"Metropolis-Hastings","text":"metropolis_hastings","category":"page"},{"location":"inference/mh/#Jaynes.metropolis_hastings","page":"Metropolis-Hastings","title":"Jaynes.metropolis_hastings","text":"call, accepted, metropolis_hastings(sel::UnconstrainedSelection,\n                                    call::HierarchicalCallSite)\n\nPerform a Metropolis-Hastings step by proposing new choices using the prior at addressed specified by sel. Returns a call site, as well as a Boolean value accepted to indicate if the proposal was accepted or rejected.\n\ncall, accepted = metropolis_hastings(sel::UnconstrainedSelection,\n                                     call::HierarchicalCallSite,\n                                     proposal::Function,\n                                     proposal_args::Tuple)\n\nPerform a Metropolis-Hastings step by proposing new choices using a custom proposal at addressed specified by sel. Returns a call site, as well as a Boolean value accepted to indicate if the proposal was accepted or rejected.\n\n\n\n\n\n","category":"function"},{"location":"inference/mh/","page":"Metropolis-Hastings","title":"Metropolis-Hastings","text":"info: Info\nSimilar to custom proposals for particle filtering, a custom proposal passed to metropolis_hastings should accept an instance of CallSite as the first argument, followed by the other proposal arguments.Additionally, if the proposal proposes to addresses which modify the control flow of the original call, it must also provide proposal choice sites for any addressed which exist on that branch of the original model. If this criterion is not satisfied, the kernel is not stationary with respect to the target posterior of the model.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"This is the documentation for the Jaynes probabilistic programming system.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"If you are familiar with probabilistic programming, you might start with Modeling Language, move into Architecture, and then into the execution contexts or inference sections in the library API reference.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"If you're new to probabilistic programming, you might start with Concepts, which provides some background about probabilistic programming, as well as some of the key conceptual ideas of this library.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Bon appetit!","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"info: Info\nThis library used to be called Walkman.jl because the original implementation used a beautiful compiler metaprogramming packaged called Cassette.jl, hence the punny name and colored cartoon Walkman logo. The original logo artwork came from an interesting article about the late and great musical device.","category":"page"},{"location":"library_api/fmi/","page":"Foreign model interface","title":"Foreign model interface","text":"CurrentModule = Jaynes","category":"page"},{"location":"library_api/fmi/","page":"Foreign model interface","title":"Foreign model interface","text":"Due to the design and implementation as an IR metaprogramming tool, Jaynes sits at a slightly privileged place in the probabilistic programming ecosystem, in the sense that many of the other languages which users are likely to use require the usage of macros to setup code in a way which allows the necessary state to be inserted for probabilistic programming functionality.","category":"page"},{"location":"library_api/fmi/","page":"Foreign model interface","title":"Foreign model interface","text":"Jaynes sees all the code after macro expansion is completed, which allows Jaynes to introspect function call sites after state has been inserted by other libraries. This allows the possibility for Jaynes to construct special call sites to represent calls into other probabilistic programming libraries. These interfaces are a work in progress, but Jaynes should theoretically provide a lingua franca for programs expressed in different probabilistic programming systems to communicate in a natural way, due to the nature of the context-oriented programming style facilitated by the system.","category":"page"},{"location":"library_api/fmi/#Black-box-extensions","page":"Foreign model interface","title":"Black-box extensions","text":"","category":"section"},{"location":"library_api/fmi/","page":"Foreign model interface","title":"Foreign model interface","text":"primitive\nforeign\nload_soss_fmi\nload_gen_fmi","category":"page"},{"location":"library_api/fmi/#Jaynes.primitive","page":"Foreign model interface","title":"Jaynes.primitive","text":"@primitive function logpdf(fn::typeof(foo), args, foo_ret)\n    ...\nend\n\n@primitive is a convenience metaprogramming construct which derives contextual dispatch definitions for functions which you'd like the tracer to \"summarize\" to a single site. This is one mechanism which gives the user more control over the structure of the choice map structure in their programs.\n\nExample:\n\ngeo(p::Float64) = rand(:flip, Bernoulli(p)) ? 1 : 1 + rand(:geo, geo, p)\n\n# Define as primitive.\n@primitive function logpdf(fn::typeof(geo), p, count)\n    return Distributions.logpdf(Geometric(p), count)\nend\n\nfunction foo()\n    ret = rand(:geo, geo, 0.3)\n    ret\nend\n\nThe above program would summarize the trace into a single choice site for :geo, as if geo was a primitive distribution.\n\n  __________________________________\n\n               Addresses\n\n geo : 42\n  __________________________________\n\n\n\n\n\n","category":"function"},{"location":"library_api/fmi/#Jaynes.foreign","page":"Foreign model interface","title":"Jaynes.foreign","text":"foreign(addr::A, m, args...) where A <: Address\n\nActivate a foreign model interface. The tracer will treat this is a specialized call site, depending on the type of m. Currently supports typeof(m) <: Soss.Model and typeof(m) <: Gen.GenerativeFunction.\n\n\n\n\n\n","category":"function"},{"location":"library_api/fmi/#Jaynes.load_soss_fmi","page":"Foreign model interface","title":"Jaynes.load_soss_fmi","text":"Jaynes.@load_soss_fmi\n\n@load_soss_fmi loads the Soss.jl foreign model interface extension. This allows you to utilize Soss models in your modeling.\n\nExample:\n\nJaynes.@load_soss_fmi()\n\n# A Soss model.\nm = @model Ïƒ begin\n    Î¼ ~ Normal()\n    y ~ Normal(Î¼, Ïƒ) |> iid(5)\nend\n\nbar = () -> begin\n    x = rand(:x, Normal(5.0, 1.0))\n    soss_ret = foreign(:foo, m, (Ïƒ = x,))\n    return soss_ret\nend\n\nThis interface currently supports all the inference interfaces (e.g. simulate, generate, score, regenerate, update, propose) which means that you can use any of the inference algorithms in the standard inference library.\n\n\n\n\n\n","category":"function"},{"location":"library_api/fmi/#Jaynes.load_gen_fmi","page":"Foreign model interface","title":"Jaynes.load_gen_fmi","text":"Jaynes.@load_gen_fmi\n\n@load_gen_fmi loads the Gen.jl foreign model interface extension. This allows you to utilize Gen models (in any of Gen's DSLs) in your modeling.\n\nExample:\n\nJaynes.@load_gen_fmi()\n\n@gen (static) function foo(z::Float64)\n    x = @trace(normal(z, 1.0), :x)\n    y = @trace(normal(x, 1.0), :y)\n    return x\nend\n\nGen.load_generated_functions()\n\nbar = () -> begin\n    x = rand(:x, Normal(0.0, 1.0))\n    return foreign(:foo, foo, x)\nend\n\nret, cl = Jaynes.simulate(bar)\n\nThis interface currently supports all the inference interfaces (e.g. simulate, generate, score, regenerate, update, propose) which means that you can use any of the inference algorithms in the standard inference library.\n\n\n\n\n\n","category":"function"}]
}
